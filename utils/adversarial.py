import torch
from tqdm import tqdm

# Compute valid normalized range for MNIST based on the transforms:
# transforms.Normalize((0.1307,), (0.3081,))
norm_min_mnist = (0 - 0.1307) / 0.3081  # ≈ -0.424
norm_max_mnist = (1 - 0.1307) / 0.3081   # ≈ 2.822

cifar_mean = (0.4914, 0.4822, 0.4465)
cifar_std = (0.2023, 0.1994, 0.2010)
norm_min_cifar = torch.tensor([(0 - m) / s for m, s in zip(cifar_mean, cifar_std)])  # shape: (3,)
norm_max_cifar = torch.tensor([(1 - m) / s for m, s in zip(cifar_mean, cifar_std)])  # shape: (3,)
# Reshape to (3, 1, 1) for broadcasting with image tensors of shape (B, 3, H, W)
norm_max_cifar = norm_max_cifar.view(3, 1, 1)
norm_max_cifar = norm_max_cifar.view(3, 1, 1)


# Evaluation function for clean (original) images.
def test_accuracy(model, test_loader, device):
    model.eval()  # Set the model to evaluation mode.
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in tqdm(test_loader, desc="Evaluating Test Acc", unit="batch"):
            data, target = data.to(device), target.to(device)
            outputs = model(data)
            # Get the index of the max log-probability.
            _, predicted = torch.max(outputs.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    return correct / total

# Evaluation function for adversarial examples generated by a given attack.
def test_accuracy_attack(model, test_loader, dataset, loss_fn, attack, attack_params, device, loss_type='cross_entropy'):
    model.eval()  # Ensure the model is in evaluation mode.
    correct = 0
    total = 0
    for data, target in tqdm(test_loader, desc="Evaluating Adversarial Acc", unit="batch"):
        data, target = data.to(device), target.to(device)
        # Generate the adversarial examples using the specified attack.
        adv_data = attack(model, loss_fn, data, target, dataset, **attack_params, loss_type=loss_type)
        outputs = model(adv_data)
        _, predicted = torch.max(outputs.data, 1)
        total += target.size(0)
        correct += (predicted == target).sum().item()
    return correct / total

# Example FGSM attack function.
def fgsm_attack(model, loss_fn, image, label, dataset, epsilon, loss_type='cross_entropy'):
    image.requires_grad = True

    if loss_type in ['multi_layer_margin', 'true_multi_layer_margin']:
        output, activations = model(image, return_activations=True)
        loss = loss_fn(output, label, activations)
    else:
        output = model(image)
        loss = loss_fn(output, label)
        
    loss.backward()
    sign_data_grad = image.grad.data.sign()
    perturbed_image = image + epsilon * sign_data_grad
    # Clamp the perturbed image to remain in the normalized valid range.
    if dataset == 'mnist':
        perturbed_image = torch.clamp(perturbed_image, norm_min_mnist, norm_max_mnist)
    elif dataset == 'cifar10':
        perturbed_image = torch.clamp(perturbed_image, norm_min_cifar, norm_max_cifar)
    else:
        raise ValueError("Unsupported dataset. Choose either 'mnist' or 'cifar10'.")
    return perturbed_image

# Example I-FGSM (Iterative FGSM) attack function.
def ifgsm_attack(model, loss_fn, image, label, dataset, epsilon, alpha, num_iter, loss_type='cross_entropy'):
    perturbed_image = image.clone()
    for i in range(num_iter):
        perturbed_image.requires_grad = True
        if loss_type in ['multi_layer_margin', 'true_multi_layer_margin']:
            output, activations = model(perturbed_image, return_activations=True)
            loss = loss_fn(output, label, activations)
        else:
            output = model(perturbed_image)
            loss = loss_fn(output, label)
        loss.backward()
        grad_sign = perturbed_image.grad.data.sign()
        # Update adversarial example with a small step.
        perturbed_image = perturbed_image + alpha * grad_sign
        # Project the perturbation to ensure it is within the epsilon ball of the original image.
        perturbation = torch.clamp(perturbed_image - image, min=-epsilon, max=epsilon)
        # Ensure the perturbed image is within the valid normalized range.
        perturbed_image = image + perturbation
        if dataset == 'mnist':
            perturbed_image = torch.clamp(perturbed_image, norm_min_mnist, norm_max_mnist)
        elif dataset == 'cifar10':
            perturbed_image = torch.clamp(perturbed_image, norm_min_cifar, norm_max_cifar)
        else:
            raise ValueError("Unsupported dataset. Choose either 'mnist' or 'cifar10'.")
    # Ensure the perturbed image is within the valid normalized range.
    return perturbed_image

